{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Isabelle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Isabelle/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# nltk sentiment analysis\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# textblob sentiment analysis\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# word processing using gensim packages\n",
    "#!pip install gensim\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "#!python -m gensim.scripts.make_wiki\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "data_path = \"~/Desktop/NewsGenerator/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load all data into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(data_path+\"articles1.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df2 = pd.read_csv(data_path+\"articles2.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "df3 = pd.read_csv(data_path+\"articles3.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 142570 entries, 0 to 42570\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   id           142570 non-null  int64  \n",
      " 1   title        142568 non-null  object \n",
      " 2   publication  142570 non-null  object \n",
      " 3   author       126694 non-null  object \n",
      " 4   date         139929 non-null  object \n",
      " 5   year         139929 non-null  float64\n",
      " 6   month        139929 non-null  float64\n",
      " 7   url          85559 non-null   object \n",
      " 8   content      142570 non-null  object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Publications\n",
    "\n",
    "we first need to figure out what types of articles are there. So, we can print the unique names of publications and how to define the project better. A couple rudimentary analysis is done to get basic statistics like counts of publications, and how I've divided up the publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Atlantic', 'Breitbart', 'Business Insider', 'Buzzfeed News',\n",
       "       'CNN', 'Fox News', 'Guardian', 'NPR', 'National Review',\n",
       "       'New York Post', 'New York Times', 'Reuters',\n",
       "       'Talking Points Memo', 'Vox', 'Washington Post'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubs = np.unique(df.publication)\n",
    "pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlantic\n",
      "\tNumber of Articles: 7179\n",
      "\tFraction of the dataset: 5.04\n",
      "Breitbart\n",
      "\tNumber of Articles: 23781\n",
      "\tFraction of the dataset: 16.68\n",
      "Business Insider\n",
      "\tNumber of Articles: 6757\n",
      "\tFraction of the dataset: 4.74\n",
      "Buzzfeed News\n",
      "\tNumber of Articles: 4854\n",
      "\tFraction of the dataset: 3.4\n",
      "CNN\n",
      "\tNumber of Articles: 11488\n",
      "\tFraction of the dataset: 8.06\n",
      "Fox News\n",
      "\tNumber of Articles: 4354\n",
      "\tFraction of the dataset: 3.05\n",
      "Guardian\n",
      "\tNumber of Articles: 8681\n",
      "\tFraction of the dataset: 6.09\n",
      "NPR\n",
      "\tNumber of Articles: 11992\n",
      "\tFraction of the dataset: 8.41\n",
      "National Review\n",
      "\tNumber of Articles: 6203\n",
      "\tFraction of the dataset: 4.35\n",
      "New York Post\n",
      "\tNumber of Articles: 17493\n",
      "\tFraction of the dataset: 12.27\n",
      "New York Times\n",
      "\tNumber of Articles: 7803\n",
      "\tFraction of the dataset: 5.47\n",
      "Reuters\n",
      "\tNumber of Articles: 10710\n",
      "\tFraction of the dataset: 7.51\n",
      "Talking Points Memo\n",
      "\tNumber of Articles: 5214\n",
      "\tFraction of the dataset: 3.66\n",
      "Vox\n",
      "\tNumber of Articles: 4947\n",
      "\tFraction of the dataset: 3.47\n",
      "Washington Post\n",
      "\tNumber of Articles: 11114\n",
      "\tFraction of the dataset: 7.8\n"
     ]
    }
   ],
   "source": [
    "total_count = 142570\n",
    "for x in pubs:\n",
    "    print(x)\n",
    "    n = df[df.publication == x].shape[0]\n",
    "    print(\"\\tNumber of Articles:\",n)\n",
    "    print(\"\\tFraction of the dataset:\",round(n/total_count*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23781"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.publication == \"Breitbart\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.publication == \"Fox News\"].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first take a look at NYT article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rift Between Officers and Residents as Killings Persist in South Bronx - The New York Times'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                         17284\n",
       "title          Rift Between Officers and Residents as Killing...\n",
       "publication                                       New York Times\n",
       "author                             Benjamin Mueller and Al Baker\n",
       "date                                                  2017-06-19\n",
       "year                                                        2017\n",
       "month                                                          6\n",
       "url                                                          NaN\n",
       "content        After the bullet shells get counted, the blood...\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt = df.iloc[1]\n",
    "nyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.157, 'neu': 0.784, 'pos': 0.059, 'compound': -1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(nyt.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we take a look at a random Breitbart article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Watch: Spicer Asked How It Feels ’To Work for a Fascist?’ In Apple Store - Breitbart'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[10001].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                         28737\n",
       "title          Watch: Spicer Asked How It Feels ’To Work for ...\n",
       "publication                                            Breitbart\n",
       "author                                              Ian Hanchett\n",
       "date                                                  2017-03-12\n",
       "year                                                        2017\n",
       "month                                                          3\n",
       "url                                                          NaN\n",
       "content        Asking @PressSec questions in Apple Store sinc...\n",
       "Name: 10001, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = df.iloc[10001]\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.156, 'neu': 0.755, 'pos': 0.089, 'compound': -0.9282}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid.polarity_scores(bb.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite such a different content and political polemics, the sentiment actually doesn't seem to have changed all that much.  Mostly, the articles both point to neutral.\n",
    "\n",
    "What if we wanted to use a different sentiment analysis tool?\n",
    "\n",
    "Let's try textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_nyt = TextBlob(nyt.content)\n",
    "blob_bb = TextBlob(bb.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.0017503893246467515, subjectivity=0.3917658789688492)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_nyt.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.028571428571428574, subjectivity=0.3380952380952381)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_bb.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a random breitbart article is more polar than a random nyt article. How about we try a couple more?\n",
    "\n",
    "Breitbart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb1 = df.iloc[10003]\n",
    "bb2 = df.iloc[10005]\n",
    "bb3 = df.iloc[10007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.06732348111658457, subjectivity=0.45755336617405584)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(bb1.content).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.09898932506887052, subjectivity=0.35658057851239666)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(bb2.content).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.0142857142857143, subjectivity=0.7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(bb3.content).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Noonan: America Is Not Obsessed With the ’Comey Drama’ Like DC - Breitbart'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb3.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.1297952794444023, subjectivity=0.43564610178645263)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt1 = df.iloc[3]\n",
    "nyt2 = df.iloc[5]\n",
    "nyt3 = df.iloc[7]\n",
    "\n",
    "TextBlob(nyt1.content).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.0678030303030303, subjectivity=0.5378787878787878)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(nyt2.content).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.062423583212209866, subjectivity=0.45431057616679965)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(nyt3.content).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Among Deaths in 2016, a Heavy Toll in Pop Music - The New York Times'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt1.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there isn't too much difference in textblob's sentiment analysis either, just from random samples of a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Clustering with LDA\n",
    "\n",
    "First we divide up the large dataset by the names of publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Atlantic', 'Breitbart', 'Business Insider', 'Buzzfeed News',\n",
       "       'CNN', 'Fox News', 'Guardian', 'NPR', 'National Review',\n",
       "       'New York Post', 'New York Times', 'Reuters',\n",
       "       'Talking Points Memo', 'Vox', 'Washington Post'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubs_df = {}\n",
    "\n",
    "for x in pubs:\n",
    "    pubs_df[x] = df[df.publication == x].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = pubs_df[\"Breitbart\"].reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = bb.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def corpus_process(corpus):\n",
    "    return nltk.tokenize.sent_tokenize(corpus)\n",
    "\n",
    "def sentence_process(processed_corpus):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in processed_corpus:\n",
    "        tokenized_sentences.append(simple_preprocess(sentence, deacc=True))\n",
    "    return tokenized_sentences\n",
    "\n",
    "def process_entire_publication(pub):\n",
    "    processed_pubs = []\n",
    "    \n",
    "    for article in pub:\n",
    "        processed_corpus = corpus_process(article)\n",
    "        tokenized_corpus = sentence_process(processed_corpus)\n",
    "        processed_pubs.append(tokenized_corpus)\n",
    "    \n",
    "    return processed_pubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bb = process_entire_publication(bb)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_corpus(processed_pub):\n",
    "    corpus = []\n",
    "\n",
    "    for articles in processed_pub:\n",
    "        a = []\n",
    "        for s in articles:\n",
    "            for t in s:\n",
    "                if t not in stop_words:\n",
    "                    a.append(t)\n",
    "        corpus.append(a)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = compose_corpus(processed_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\\\n",
    "                                           id2word=id2word,\\\n",
    "                                           num_topics=20, \\\n",
    "                                           random_state=0,\\\n",
    "                                           update_every=1,\\\n",
    "                                           chunksize=100,\\\n",
    "                                           passes=10,\\\n",
    "                                           alpha='auto',\\\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.048*\"muslim\" + 0.042*\"islamic\" + 0.027*\"muslims\" + 0.025*\"attack\" + 0.023*\"attacks\" + 0.023*\"terrorist\" + 0.022*\"islam\" + 0.018*\"israel\" + 0.017*\"terror\" + 0.014*\"state\"'),\n",
       " (1,\n",
       "  '0.120*\"women\" + 0.038*\"men\" + 0.032*\"children\" + 0.031*\"woman\" + 0.027*\"sexual\" + 0.026*\"child\" + 0.023*\"sex\" + 0.022*\"planned\" + 0.022*\"abortion\" + 0.019*\"cent\"'),\n",
       " (2,\n",
       "  '0.039*\"said\" + 0.028*\"people\" + 0.017*\"going\" + 0.016*\"think\" + 0.016*\"like\" + 0.015*\"would\" + 0.014*\"get\" + 0.013*\"know\" + 0.011*\"want\" + 0.011*\"say\"'),\n",
       " (3,\n",
       "  '0.054*\"mr\" + 0.044*\"migrants\" + 0.033*\"party\" + 0.031*\"european\" + 0.031*\"eu\" + 0.028*\"europe\" + 0.025*\"london\" + 0.023*\"britain\" + 0.021*\"british\" + 0.018*\"german\"'),\n",
       " (4,\n",
       "  '0.092*\"school\" + 0.046*\"education\" + 0.045*\"students\" + 0.034*\"common\" + 0.033*\"core\" + 0.032*\"virus\" + 0.032*\"schools\" + 0.030*\"cases\" + 0.027*\"records\" + 0.021*\"district\"'),\n",
       " (5,\n",
       "  '0.154*\"china\" + 0.067*\"wikileaks\" + 0.061*\"chinese\" + 0.027*\"lucas\" + 0.025*\"vietnam\" + 0.024*\"revelations\" + 0.019*\"sea\" + 0.018*\"nsa\" + 0.017*\"briefing\" + 0.015*\"artificial\"'),\n",
       " (6,\n",
       "  '0.019*\"obama\" + 0.015*\"president\" + 0.014*\"would\" + 0.011*\"american\" + 0.010*\"states\" + 0.009*\"immigration\" + 0.009*\"government\" + 0.008*\"law\" + 0.008*\"also\" + 0.008*\"united\"'),\n",
       " (7,\n",
       "  '0.107*\"news\" + 0.065*\"fox\" + 0.061*\"breitbart\" + 0.033*\"daily\" + 0.025*\"show\" + 0.022*\"host\" + 0.022*\"siriusxm\" + 0.020*\"kelly\" + 0.018*\"channel\" + 0.015*\"live\"'),\n",
       " (8,\n",
       "  '0.099*\"gun\" + 0.046*\"amendment\" + 0.040*\"breitbart\" + 0.035*\"awrhawkins\" + 0.032*\"second\" + 0.031*\"control\" + 0.029*\"news\" + 0.026*\"armed\" + 0.023*\"hawkins\" + 0.020*\"awr\"'),\n",
       " (9,\n",
       "  '0.058*\"percent\" + 0.022*\"million\" + 0.018*\"poll\" + 0.016*\"trade\" + 0.013*\"jobs\" + 0.013*\"workers\" + 0.011*\"year\" + 0.010*\"according\" + 0.010*\"tax\" + 0.010*\"new\"'),\n",
       " (10,\n",
       "  '0.154*\"clinton\" + 0.066*\"hillary\" + 0.021*\"state\" + 0.017*\"campaign\" + 0.017*\"email\" + 0.015*\"secretary\" + 0.013*\"fbi\" + 0.013*\"bill\" + 0.012*\"democratic\" + 0.011*\"former\"'),\n",
       " (11,\n",
       "  '0.037*\"media\" + 0.036*\"news\" + 0.036*\"new\" + 0.033*\"breitbart\" + 0.023*\"york\" + 0.017*\"times\" + 0.017*\"post\" + 0.016*\"story\" + 0.015*\"facebook\" + 0.011*\"book\"'),\n",
       " (12,\n",
       "  '0.074*\"health\" + 0.042*\"klein\" + 0.026*\"aaron\" + 0.025*\"medical\" + 0.025*\"radio\" + 0.024*\"obamacare\" + 0.022*\"cuba\" + 0.021*\"jerusalem\" + 0.019*\"care\" + 0.019*\"program\"'),\n",
       " (13,\n",
       "  '0.047*\"says\" + 0.040*\"twitter\" + 0.018*\"pic\" + 0.015*\"july\" + 0.014*\"march\" + 0.011*\"https\" + 0.009*\"convention\" + 0.008*\"rally\" + 0.008*\"february\" + 0.008*\"show\"'),\n",
       " (14,\n",
       "  '0.021*\"one\" + 0.011*\"time\" + 0.011*\"years\" + 0.010*\"first\" + 0.009*\"two\" + 0.009*\"many\" + 0.009*\"last\" + 0.008*\"year\" + 0.008*\"even\" + 0.008*\"world\"'),\n",
       " (15,\n",
       "  '0.059*\"black\" + 0.033*\"university\" + 0.027*\"twitter\" + 0.024*\"milo\" + 0.022*\"white\" + 0.022*\"speech\" + 0.021*\"lives\" + 0.020*\"matter\" + 0.018*\"college\" + 0.017*\"yiannopoulos\"'),\n",
       " (16,\n",
       "  '0.184*\"trump\" + 0.051*\"donald\" + 0.034*\"cruz\" + 0.034*\"republican\" + 0.028*\"campaign\" + 0.024*\"presidential\" + 0.022*\"party\" + 0.020*\"election\" + 0.020*\"sanders\" + 0.020*\"vote\"'),\n",
       " (17,\n",
       "  '0.018*\"refugees\" + 0.018*\"said\" + 0.017*\"isis\" + 0.016*\"security\" + 0.014*\"state\" + 0.014*\"military\" + 0.013*\"iran\" + 0.013*\"united\" + 0.011*\"minister\" + 0.010*\"syria\"'),\n",
       " (18,\n",
       "  '0.047*\"police\" + 0.019*\"said\" + 0.015*\"man\" + 0.012*\"city\" + 0.012*\"told\" + 0.011*\"officers\" + 0.010*\"protesters\" + 0.009*\"two\" + 0.009*\"video\" + 0.008*\"shot\"'),\n",
       " (19,\n",
       "  '0.039*\"texas\" + 0.028*\"border\" + 0.025*\"breitbart\" + 0.021*\"illegal\" + 0.019*\"county\" + 0.016*\"reported\" + 0.016*\"enforcement\" + 0.012*\"law\" + 0.011*\"officials\" + 0.010*\"prison\"')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
